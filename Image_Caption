{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1482460,"sourceType":"datasetVersion","datasetId":870054},{"sourceId":11595162,"sourceType":"datasetVersion","datasetId":7271208},{"sourceId":11663777,"sourceType":"datasetVersion","datasetId":7319993}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:37.428570Z","iopub.execute_input":"2025-05-03T16:20:37.428830Z","iopub.status.idle":"2025-05-03T16:20:41.380135Z","shell.execute_reply.started":"2025-05-03T16:20:37.428804Z","shell.execute_reply":"2025-05-03T16:20:41.379411Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport nltk\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:41.380941Z","iopub.execute_input":"2025-05-03T16:20:41.381323Z","iopub.status.idle":"2025-05-03T16:20:45.766585Z","shell.execute_reply.started":"2025-05-03T16:20:41.381245Z","shell.execute_reply":"2025-05-03T16:20:45.766045Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:45.767900Z","iopub.execute_input":"2025-05-03T16:20:45.768251Z","iopub.status.idle":"2025-05-03T16:20:46.045613Z","shell.execute_reply.started":"2025-05-03T16:20:45.768231Z","shell.execute_reply":"2025-05-03T16:20:46.044994Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.046287Z","iopub.execute_input":"2025-05-03T16:20:46.046550Z","iopub.status.idle":"2025-05-03T16:20:46.049991Z","shell.execute_reply.started":"2025-05-03T16:20:46.046532Z","shell.execute_reply":"2025-05-03T16:20:46.049479Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.050655Z","iopub.execute_input":"2025-05-03T16:20:46.051035Z","iopub.status.idle":"2025-05-03T16:20:46.064197Z","shell.execute_reply.started":"2025-05-03T16:20:46.051013Z","shell.execute_reply":"2025-05-03T16:20:46.063529Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold=1):\n        self.freq_threshold = freq_threshold\n        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.word_freq = {}\n        self.idx = 4\n\n    def build_vocabulary(self, sentence_list):\n        for sentence in sentence_list:\n            for word in word_tokenize(sentence.lower()):\n                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n                if self.word_freq[word] == self.freq_threshold:\n                    self.word2idx[word] = self.idx\n                    self.idx2word[self.idx] = word\n                    self.idx += 1\n\n    def numericalize(self, text):\n        tokenized = word_tokenize(text.lower())\n        return [self.word2idx.get(word, self.word2idx[\"<UNK>\"]) for word in tokenized]\n\n    def __call__(self, word):\n        return self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n\n    def __len__(self):\n        return len(self.word2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.064933Z","iopub.execute_input":"2025-05-03T16:20:46.065162Z","iopub.status.idle":"2025-05-03T16:20:46.077303Z","shell.execute_reply.started":"2025-05-03T16:20:46.065148Z","shell.execute_reply":"2025-05-03T16:20:46.076770Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class CaptionDataset(Dataset):\n    def __init__(self, image_folder, captions_file, vocab, transform=None):\n        self.captions_data = self._load_captions(captions_file)\n        self.image_folder = image_folder\n        self.transform = transform\n        self.vocab = vocab\n        self.data = [(img, cap) for img, caps in self.captions_data.items() for cap in caps]\n\n    def _load_captions(self, captions_file):\n        all_captions = collections.defaultdict(list)\n        with open(captions_file, 'r', newline='') as csvfile:\n                reader = csv.DictReader(csvfile)\n                for row in reader:\n                    image_id = row['image']\n                    caption = row['caption']\n                    all_captions[image_id].append(caption)\n        return dict(all_captions)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name, caption = self.data[idx]\n        image = Image.open(os.path.join(self.image_folder, img_name)).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n\n        numericalized_caption = [self.vocab(\"<SOS>\")] + self.vocab.numericalize(caption) + [self.vocab(\"<EOS>\")]\n        return image, torch.tensor(numericalized_caption)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.078033Z","iopub.execute_input":"2025-05-03T16:20:46.078274Z","iopub.status.idle":"2025-05-03T16:20:46.091795Z","shell.execute_reply.started":"2025-05-03T16:20:46.078240Z","shell.execute_reply":"2025-05-03T16:20:46.091107Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1] # remove the final classification layer\n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n\n    def forward(self, images):\n        with torch.no_grad():\n            features = self.resnet(images).squeeze()\n        features = self.linear(features)\n        features = self.bn(features)\n        return features ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.092479Z","iopub.execute_input":"2025-05-03T16:20:46.092702Z","iopub.status.idle":"2025-05-03T16:20:46.109311Z","shell.execute_reply.started":"2025-05-03T16:20:46.092676Z","shell.execute_reply":"2025-05-03T16:20:46.108659Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class DecoderLSTM(nn.Module): \n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) \n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions):\n        embeddings = self.embed(captions[:, :-1])\n        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n        hiddens, _ = self.lstm(inputs)\n        outputs = self.linear(hiddens)\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.111241Z","iopub.execute_input":"2025-05-03T16:20:46.111461Z","iopub.status.idle":"2025-05-03T16:20:46.123815Z","shell.execute_reply.started":"2025-05-03T16:20:46.111447Z","shell.execute_reply":"2025-05-03T16:20:46.123229Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def generate_caption(encoder, decoder, image, vocab, max_len=20):\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad():\n        feature = encoder(image.unsqueeze(0))\n        inputs = feature\n        caption = []\n        states = None\n        input_word = torch.tensor([[vocab('<SOS>')]])\n        for _ in range(max_len):\n            embedded = decoder.embed(input_word).squeeze(1)\n            inputs, states = decoder.lstm(embedded.unsqueeze(1), states) \n            outputs = decoder.linear(inputs.squeeze(1))\n            predicted = outputs.argmax(1)\n            word = vocab.idx2word[predicted.item()]\n            if word == '<EOS>':\n                break\n            caption.append(word)\n            input_word = predicted.unsqueeze(0)\n    return ' '.join(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.124454Z","iopub.execute_input":"2025-05-03T16:20:46.124640Z","iopub.status.idle":"2025-05-03T16:20:46.142875Z","shell.execute_reply.started":"2025-05-03T16:20:46.124626Z","shell.execute_reply":"2025-05-03T16:20:46.142142Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def train(encoder, decoder, dataloader, criterion, optimizer, vocab, device):\n    encoder.train()\n    decoder.train()\n    for imgs, captions, lengths in dataloader:\n        imgs, captions = imgs.to(device), captions.to(device)\n        features = encoder(imgs)\n        outputs = decoder(features, captions[:, :-1]) \n        targets = captions[:, 1:] \n        \n        outputs = outputs.reshape(-1, outputs.shape[2])\n        targets = targets.reshape(-1)\n\n        loss = criterion(outputs, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.143685Z","iopub.execute_input":"2025-05-03T16:20:46.143912Z","iopub.status.idle":"2025-05-03T16:20:46.157394Z","shell.execute_reply.started":"2025-05-03T16:20:46.143892Z","shell.execute_reply":"2025-05-03T16:20:46.156685Z"}},"outputs":[],"execution_count":11},{"cell_type":"raw","source":"import collection","metadata":{}},{"cell_type":"code","source":"import collections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:20:46.158164Z","iopub.execute_input":"2025-05-03T16:20:46.158399Z","iopub.status.idle":"2025-05-03T16:20:46.171009Z","shell.execute_reply.started":"2025-05-03T16:20:46.158380Z","shell.execute_reply":"2025-05-03T16:20:46.170470Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Parameters\nembed_size = 256\nhidden_size = 512\nnum_epochs = 50\nbatch_size = 32\nlearning_rate = 3e-4\n\n# Image transform\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\nimage_folder = \"/kaggle/input/flickr-8k-images-with-captions/Images\"  \ncaptions_file = \"/kaggle/input/flickr-8k-images-with-captions/captions.txt\" \n\nall_captions = collections.defaultdict(list)\nwith open(captions_file, 'r', newline='') as txtfile:\n    reader = csv.DictReader(txtfile)\n    for row in reader:\n        image_id = row['image']\n        caption = row['caption']\n        all_captions[image_id].append(caption)\n\nflat_captions = [c for caps in all_captions.values() for c in caps]\n\nvocab = Vocabulary()\nvocab.build_vocabulary(flat_captions)\n\ndef collate_fn(batch):\n    # Separate images and captions\n    images = [item[0] for item in batch]\n    captions = [item[1] for item in batch]\n\n    images = torch.stack(images, 0)\n\n    # Pad captions\n    lengths = [len(cap) for cap in captions]\n    padded_captions = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        padded_captions[i, :end] = cap[:end]\n\n    return images, padded_captions, torch.tensor(lengths)\n\n\ndataset = CaptionDataset(image_folder, captions_file, vocab, transform)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderLSTM(embed_size, hidden_size, len(vocab)).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=vocab(\"<PAD>\"))\nparams = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\noptimizer = torch.optim.Adam(params, lr=learning_rate)\n\n\nfor epoch in range(num_epochs):\n    for i, (imgs, captions, lengths) in enumerate(dataloader):\n        imgs, captions = imgs.to(device), captions.to(device)\n        features = encoder(imgs)\n        outputs = decoder(features, captions[:, :-1])\n        targets = captions[:, 1:]\n        outputs = outputs.reshape(-1, outputs.shape[2])\n        targets = targets.reshape(-1)\n        loss = criterion(outputs, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] completed.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T17:12:59.092382Z","iopub.execute_input":"2025-05-03T17:12:59.092681Z","iopub.status.idle":"2025-05-03T20:40:59.190131Z","shell.execute_reply.started":"2025-05-03T17:12:59.092653Z","shell.execute_reply":"2025-05-03T20:40:59.189366Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/50], Step [100/1265], Loss: 5.0703\nEpoch [1/50], Step [200/1265], Loss: 4.7347\nEpoch [1/50], Step [300/1265], Loss: 4.5758\nEpoch [1/50], Step [400/1265], Loss: 4.6407\nEpoch [1/50], Step [500/1265], Loss: 4.6437\nEpoch [1/50], Step [600/1265], Loss: 3.9865\nEpoch [1/50], Step [700/1265], Loss: 4.2908\nEpoch [1/50], Step [800/1265], Loss: 4.2353\nEpoch [1/50], Step [900/1265], Loss: 4.3762\nEpoch [1/50], Step [1000/1265], Loss: 4.2974\nEpoch [1/50], Step [1100/1265], Loss: 4.2806\nEpoch [1/50], Step [1200/1265], Loss: 3.9406\nEpoch [1/50] completed.\nEpoch [2/50], Step [100/1265], Loss: 3.9056\nEpoch [2/50], Step [200/1265], Loss: 4.0174\nEpoch [2/50], Step [300/1265], Loss: 3.9922\nEpoch [2/50], Step [400/1265], Loss: 4.0210\nEpoch [2/50], Step [500/1265], Loss: 3.8325\nEpoch [2/50], Step [600/1265], Loss: 3.7743\nEpoch [2/50], Step [700/1265], Loss: 4.0587\nEpoch [2/50], Step [800/1265], Loss: 3.8286\nEpoch [2/50], Step [900/1265], Loss: 3.9736\nEpoch [2/50], Step [1000/1265], Loss: 3.5259\nEpoch [2/50], Step [1100/1265], Loss: 3.7758\nEpoch [2/50], Step [1200/1265], Loss: 3.5236\nEpoch [2/50] completed.\nEpoch [3/50], Step [100/1265], Loss: 3.5032\nEpoch [3/50], Step [200/1265], Loss: 3.9008\nEpoch [3/50], Step [300/1265], Loss: 3.6949\nEpoch [3/50], Step [400/1265], Loss: 3.4199\nEpoch [3/50], Step [500/1265], Loss: 3.6576\nEpoch [3/50], Step [600/1265], Loss: 3.5272\nEpoch [3/50], Step [700/1265], Loss: 3.5777\nEpoch [3/50], Step [800/1265], Loss: 3.5125\nEpoch [3/50], Step [900/1265], Loss: 3.5809\nEpoch [3/50], Step [1000/1265], Loss: 3.4115\nEpoch [3/50], Step [1100/1265], Loss: 3.5308\nEpoch [3/50], Step [1200/1265], Loss: 3.4954\nEpoch [3/50] completed.\nEpoch [4/50], Step [100/1265], Loss: 3.6276\nEpoch [4/50], Step [200/1265], Loss: 3.3469\nEpoch [4/50], Step [300/1265], Loss: 3.3116\nEpoch [4/50], Step [400/1265], Loss: 3.0683\nEpoch [4/50], Step [500/1265], Loss: 3.4801\nEpoch [4/50], Step [600/1265], Loss: 3.5913\nEpoch [4/50], Step [700/1265], Loss: 3.5591\nEpoch [4/50], Step [800/1265], Loss: 3.2549\nEpoch [4/50], Step [900/1265], Loss: 3.4270\nEpoch [4/50], Step [1000/1265], Loss: 3.2872\nEpoch [4/50], Step [1100/1265], Loss: 3.4788\nEpoch [4/50], Step [1200/1265], Loss: 3.2591\nEpoch [4/50] completed.\nEpoch [5/50], Step [100/1265], Loss: 3.0400\nEpoch [5/50], Step [200/1265], Loss: 3.5704\nEpoch [5/50], Step [300/1265], Loss: 3.1258\nEpoch [5/50], Step [400/1265], Loss: 3.4865\nEpoch [5/50], Step [500/1265], Loss: 3.1404\nEpoch [5/50], Step [600/1265], Loss: 3.5390\nEpoch [5/50], Step [700/1265], Loss: 3.0958\nEpoch [5/50], Step [800/1265], Loss: 3.3745\nEpoch [5/50], Step [900/1265], Loss: 3.2092\nEpoch [5/50], Step [1000/1265], Loss: 3.0284\nEpoch [5/50], Step [1100/1265], Loss: 3.1776\nEpoch [5/50], Step [1200/1265], Loss: 3.3349\nEpoch [5/50] completed.\nEpoch [6/50], Step [100/1265], Loss: 2.9001\nEpoch [6/50], Step [200/1265], Loss: 2.9157\nEpoch [6/50], Step [300/1265], Loss: 3.1058\nEpoch [6/50], Step [400/1265], Loss: 3.0840\nEpoch [6/50], Step [500/1265], Loss: 3.1286\nEpoch [6/50], Step [600/1265], Loss: 2.9423\nEpoch [6/50], Step [700/1265], Loss: 2.9333\nEpoch [6/50], Step [800/1265], Loss: 3.1128\nEpoch [6/50], Step [900/1265], Loss: 3.1813\nEpoch [6/50], Step [1000/1265], Loss: 3.1190\nEpoch [6/50], Step [1100/1265], Loss: 2.9471\nEpoch [6/50], Step [1200/1265], Loss: 3.0876\nEpoch [6/50] completed.\nEpoch [7/50], Step [100/1265], Loss: 2.7024\nEpoch [7/50], Step [200/1265], Loss: 3.0518\nEpoch [7/50], Step [300/1265], Loss: 2.9280\nEpoch [7/50], Step [400/1265], Loss: 3.0002\nEpoch [7/50], Step [500/1265], Loss: 2.8663\nEpoch [7/50], Step [600/1265], Loss: 2.6273\nEpoch [7/50], Step [700/1265], Loss: 3.0790\nEpoch [7/50], Step [800/1265], Loss: 3.0036\nEpoch [7/50], Step [900/1265], Loss: 3.0825\nEpoch [7/50], Step [1000/1265], Loss: 2.9896\nEpoch [7/50], Step [1100/1265], Loss: 2.9161\nEpoch [7/50], Step [1200/1265], Loss: 2.8725\nEpoch [7/50] completed.\nEpoch [8/50], Step [100/1265], Loss: 2.8313\nEpoch [8/50], Step [200/1265], Loss: 2.9523\nEpoch [8/50], Step [300/1265], Loss: 2.8254\nEpoch [8/50], Step [400/1265], Loss: 2.7454\nEpoch [8/50], Step [500/1265], Loss: 2.8138\nEpoch [8/50], Step [600/1265], Loss: 3.1586\nEpoch [8/50], Step [700/1265], Loss: 2.8506\nEpoch [8/50], Step [800/1265], Loss: 2.9624\nEpoch [8/50], Step [900/1265], Loss: 2.9029\nEpoch [8/50], Step [1000/1265], Loss: 2.8667\nEpoch [8/50], Step [1100/1265], Loss: 2.8934\nEpoch [8/50], Step [1200/1265], Loss: 2.7107\nEpoch [8/50] completed.\nEpoch [9/50], Step [100/1265], Loss: 2.7372\nEpoch [9/50], Step [200/1265], Loss: 2.8846\nEpoch [9/50], Step [300/1265], Loss: 2.8328\nEpoch [9/50], Step [400/1265], Loss: 2.8555\nEpoch [9/50], Step [500/1265], Loss: 2.8056\nEpoch [9/50], Step [600/1265], Loss: 2.6837\nEpoch [9/50], Step [700/1265], Loss: 2.6592\nEpoch [9/50], Step [800/1265], Loss: 2.6265\nEpoch [9/50], Step [900/1265], Loss: 2.7723\nEpoch [9/50], Step [1000/1265], Loss: 2.7226\nEpoch [9/50], Step [1100/1265], Loss: 2.7774\nEpoch [9/50], Step [1200/1265], Loss: 2.6996\nEpoch [9/50] completed.\nEpoch [10/50], Step [100/1265], Loss: 2.7699\nEpoch [10/50], Step [200/1265], Loss: 2.5001\nEpoch [10/50], Step [300/1265], Loss: 2.7321\nEpoch [10/50], Step [400/1265], Loss: 2.6157\nEpoch [10/50], Step [500/1265], Loss: 2.6636\nEpoch [10/50], Step [600/1265], Loss: 2.7539\nEpoch [10/50], Step [700/1265], Loss: 2.6665\nEpoch [10/50], Step [800/1265], Loss: 2.4888\nEpoch [10/50], Step [900/1265], Loss: 2.5241\nEpoch [10/50], Step [1000/1265], Loss: 2.6359\nEpoch [10/50], Step [1100/1265], Loss: 2.7690\nEpoch [10/50], Step [1200/1265], Loss: 2.5281\nEpoch [10/50] completed.\nEpoch [11/50], Step [100/1265], Loss: 2.3420\nEpoch [11/50], Step [200/1265], Loss: 2.7549\nEpoch [11/50], Step [300/1265], Loss: 2.5573\nEpoch [11/50], Step [400/1265], Loss: 2.5133\nEpoch [11/50], Step [500/1265], Loss: 2.4971\nEpoch [11/50], Step [600/1265], Loss: 2.4716\nEpoch [11/50], Step [700/1265], Loss: 2.5618\nEpoch [11/50], Step [800/1265], Loss: 2.6889\nEpoch [11/50], Step [900/1265], Loss: 2.5918\nEpoch [11/50], Step [1000/1265], Loss: 2.5496\nEpoch [11/50], Step [1100/1265], Loss: 2.6273\nEpoch [11/50], Step [1200/1265], Loss: 2.5122\nEpoch [11/50] completed.\nEpoch [12/50], Step [100/1265], Loss: 2.2738\nEpoch [12/50], Step [200/1265], Loss: 2.5281\nEpoch [12/50], Step [300/1265], Loss: 2.3738\nEpoch [12/50], Step [400/1265], Loss: 2.6824\nEpoch [12/50], Step [500/1265], Loss: 2.3907\nEpoch [12/50], Step [600/1265], Loss: 2.5281\nEpoch [12/50], Step [700/1265], Loss: 2.6195\nEpoch [12/50], Step [800/1265], Loss: 2.5236\nEpoch [12/50], Step [900/1265], Loss: 2.5190\nEpoch [12/50], Step [1000/1265], Loss: 2.3496\nEpoch [12/50], Step [1100/1265], Loss: 2.2819\nEpoch [12/50], Step [1200/1265], Loss: 2.5570\nEpoch [12/50] completed.\nEpoch [13/50], Step [100/1265], Loss: 2.3337\nEpoch [13/50], Step [200/1265], Loss: 2.5061\nEpoch [13/50], Step [300/1265], Loss: 2.2811\nEpoch [13/50], Step [400/1265], Loss: 2.2641\nEpoch [13/50], Step [500/1265], Loss: 2.4759\nEpoch [13/50], Step [600/1265], Loss: 2.4216\nEpoch [13/50], Step [700/1265], Loss: 2.2810\nEpoch [13/50], Step [800/1265], Loss: 2.3292\nEpoch [13/50], Step [900/1265], Loss: 2.5945\nEpoch [13/50], Step [1000/1265], Loss: 2.2778\nEpoch [13/50], Step [1100/1265], Loss: 2.4191\nEpoch [13/50], Step [1200/1265], Loss: 2.4086\nEpoch [13/50] completed.\nEpoch [14/50], Step [100/1265], Loss: 2.1685\nEpoch [14/50], Step [200/1265], Loss: 2.1867\nEpoch [14/50], Step [300/1265], Loss: 2.2449\nEpoch [14/50], Step [400/1265], Loss: 2.3767\nEpoch [14/50], Step [500/1265], Loss: 2.2081\nEpoch [14/50], Step [600/1265], Loss: 2.1860\nEpoch [14/50], Step [700/1265], Loss: 2.1146\nEpoch [14/50], Step [800/1265], Loss: 2.2855\nEpoch [14/50], Step [900/1265], Loss: 2.4751\nEpoch [14/50], Step [1000/1265], Loss: 2.1940\nEpoch [14/50], Step [1100/1265], Loss: 2.3467\nEpoch [14/50], Step [1200/1265], Loss: 2.4048\nEpoch [14/50] completed.\nEpoch [15/50], Step [100/1265], Loss: 2.1527\nEpoch [15/50], Step [200/1265], Loss: 2.0312\nEpoch [15/50], Step [300/1265], Loss: 2.1858\nEpoch [15/50], Step [400/1265], Loss: 2.4095\nEpoch [15/50], Step [500/1265], Loss: 2.0263\nEpoch [15/50], Step [600/1265], Loss: 2.3442\nEpoch [15/50], Step [700/1265], Loss: 2.2094\nEpoch [15/50], Step [800/1265], Loss: 2.1980\nEpoch [15/50], Step [900/1265], Loss: 2.2755\nEpoch [15/50], Step [1000/1265], Loss: 2.3627\nEpoch [15/50], Step [1100/1265], Loss: 2.3993\nEpoch [15/50], Step [1200/1265], Loss: 2.2075\nEpoch [15/50] completed.\nEpoch [16/50], Step [100/1265], Loss: 2.0547\nEpoch [16/50], Step [200/1265], Loss: 2.0357\nEpoch [16/50], Step [300/1265], Loss: 2.2205\nEpoch [16/50], Step [400/1265], Loss: 2.1440\nEpoch [16/50], Step [500/1265], Loss: 2.0996\nEpoch [16/50], Step [600/1265], Loss: 2.2004\nEpoch [16/50], Step [700/1265], Loss: 2.0853\nEpoch [16/50], Step [800/1265], Loss: 2.2576\nEpoch [16/50], Step [900/1265], Loss: 2.1179\nEpoch [16/50], Step [1000/1265], Loss: 2.2839\nEpoch [16/50], Step [1100/1265], Loss: 2.0413\nEpoch [16/50], Step [1200/1265], Loss: 2.2676\nEpoch [16/50] completed.\nEpoch [17/50], Step [100/1265], Loss: 2.1737\nEpoch [17/50], Step [200/1265], Loss: 1.9750\nEpoch [17/50], Step [300/1265], Loss: 2.0665\nEpoch [17/50], Step [400/1265], Loss: 2.0211\nEpoch [17/50], Step [500/1265], Loss: 2.1565\nEpoch [17/50], Step [600/1265], Loss: 2.0420\nEpoch [17/50], Step [700/1265], Loss: 1.9393\nEpoch [17/50], Step [800/1265], Loss: 1.9615\nEpoch [17/50], Step [900/1265], Loss: 2.0488\nEpoch [17/50], Step [1000/1265], Loss: 2.0513\nEpoch [17/50], Step [1100/1265], Loss: 2.0144\nEpoch [17/50], Step [1200/1265], Loss: 2.1544\nEpoch [17/50] completed.\nEpoch [18/50], Step [100/1265], Loss: 2.2215\nEpoch [18/50], Step [200/1265], Loss: 1.7219\nEpoch [18/50], Step [300/1265], Loss: 2.0977\nEpoch [18/50], Step [400/1265], Loss: 2.0059\nEpoch [18/50], Step [500/1265], Loss: 1.9453\nEpoch [18/50], Step [600/1265], Loss: 1.9856\nEpoch [18/50], Step [700/1265], Loss: 1.9234\nEpoch [18/50], Step [800/1265], Loss: 2.0527\nEpoch [18/50], Step [900/1265], Loss: 2.0988\nEpoch [18/50], Step [1000/1265], Loss: 2.2057\nEpoch [18/50], Step [1100/1265], Loss: 1.9722\nEpoch [18/50], Step [1200/1265], Loss: 2.0740\nEpoch [18/50] completed.\nEpoch [19/50], Step [100/1265], Loss: 1.9568\nEpoch [19/50], Step [200/1265], Loss: 1.7132\nEpoch [19/50], Step [300/1265], Loss: 1.8742\nEpoch [19/50], Step [400/1265], Loss: 1.8630\nEpoch [19/50], Step [500/1265], Loss: 1.9449\nEpoch [19/50], Step [600/1265], Loss: 1.9351\nEpoch [19/50], Step [700/1265], Loss: 1.9739\nEpoch [19/50], Step [800/1265], Loss: 2.0641\nEpoch [19/50], Step [900/1265], Loss: 2.1408\nEpoch [19/50], Step [1000/1265], Loss: 2.0795\nEpoch [19/50], Step [1100/1265], Loss: 2.0733\nEpoch [19/50], Step [1200/1265], Loss: 1.8564\nEpoch [19/50] completed.\nEpoch [20/50], Step [100/1265], Loss: 1.6800\nEpoch [20/50], Step [200/1265], Loss: 1.7967\nEpoch [20/50], Step [300/1265], Loss: 1.8846\nEpoch [20/50], Step [400/1265], Loss: 1.9837\nEpoch [20/50], Step [500/1265], Loss: 1.9496\nEpoch [20/50], Step [600/1265], Loss: 1.9628\nEpoch [20/50], Step [700/1265], Loss: 1.9644\nEpoch [20/50], Step [800/1265], Loss: 1.9354\nEpoch [20/50], Step [900/1265], Loss: 2.0874\nEpoch [20/50], Step [1000/1265], Loss: 2.0166\nEpoch [20/50], Step [1100/1265], Loss: 1.8893\nEpoch [20/50], Step [1200/1265], Loss: 2.0426\nEpoch [20/50] completed.\nEpoch [21/50], Step [100/1265], Loss: 1.8222\nEpoch [21/50], Step [200/1265], Loss: 1.7521\nEpoch [21/50], Step [300/1265], Loss: 1.8676\nEpoch [21/50], Step [400/1265], Loss: 1.9244\nEpoch [21/50], Step [500/1265], Loss: 1.8021\nEpoch [21/50], Step [600/1265], Loss: 1.8771\nEpoch [21/50], Step [700/1265], Loss: 1.8696\nEpoch [21/50], Step [800/1265], Loss: 1.8545\nEpoch [21/50], Step [900/1265], Loss: 2.0005\nEpoch [21/50], Step [1000/1265], Loss: 1.9384\nEpoch [21/50], Step [1100/1265], Loss: 1.7975\nEpoch [21/50], Step [1200/1265], Loss: 1.9202\nEpoch [21/50] completed.\nEpoch [22/50], Step [100/1265], Loss: 1.7262\nEpoch [22/50], Step [200/1265], Loss: 1.6532\nEpoch [22/50], Step [300/1265], Loss: 1.8713\nEpoch [22/50], Step [400/1265], Loss: 1.7550\nEpoch [22/50], Step [500/1265], Loss: 1.8791\nEpoch [22/50], Step [600/1265], Loss: 1.8109\nEpoch [22/50], Step [700/1265], Loss: 2.0356\nEpoch [22/50], Step [800/1265], Loss: 1.8039\nEpoch [22/50], Step [900/1265], Loss: 1.8304\nEpoch [22/50], Step [1000/1265], Loss: 2.0261\nEpoch [22/50], Step [1100/1265], Loss: 1.9506\nEpoch [22/50], Step [1200/1265], Loss: 1.7973\nEpoch [22/50] completed.\nEpoch [23/50], Step [100/1265], Loss: 1.6962\nEpoch [23/50], Step [200/1265], Loss: 1.7008\nEpoch [23/50], Step [300/1265], Loss: 1.7429\nEpoch [23/50], Step [400/1265], Loss: 1.8177\nEpoch [23/50], Step [500/1265], Loss: 1.8728\nEpoch [23/50], Step [600/1265], Loss: 1.7947\nEpoch [23/50], Step [700/1265], Loss: 1.6135\nEpoch [23/50], Step [800/1265], Loss: 1.6388\nEpoch [23/50], Step [900/1265], Loss: 1.5838\nEpoch [23/50], Step [1000/1265], Loss: 1.5519\nEpoch [23/50], Step [1100/1265], Loss: 1.7518\nEpoch [23/50], Step [1200/1265], Loss: 1.7107\nEpoch [23/50] completed.\nEpoch [24/50], Step [100/1265], Loss: 1.6067\nEpoch [24/50], Step [200/1265], Loss: 1.5993\nEpoch [24/50], Step [300/1265], Loss: 1.5898\nEpoch [24/50], Step [400/1265], Loss: 1.5818\nEpoch [24/50], Step [500/1265], Loss: 1.7873\nEpoch [24/50], Step [600/1265], Loss: 1.7332\nEpoch [24/50], Step [700/1265], Loss: 1.6707\nEpoch [24/50], Step [800/1265], Loss: 1.7044\nEpoch [24/50], Step [900/1265], Loss: 1.7198\nEpoch [24/50], Step [1000/1265], Loss: 1.7932\nEpoch [24/50], Step [1100/1265], Loss: 1.6313\nEpoch [24/50], Step [1200/1265], Loss: 1.6322\nEpoch [24/50] completed.\nEpoch [25/50], Step [100/1265], Loss: 1.6267\nEpoch [25/50], Step [200/1265], Loss: 1.5846\nEpoch [25/50], Step [300/1265], Loss: 1.5601\nEpoch [25/50], Step [400/1265], Loss: 1.7479\nEpoch [25/50], Step [500/1265], Loss: 1.5875\nEpoch [25/50], Step [600/1265], Loss: 1.6297\nEpoch [25/50], Step [700/1265], Loss: 1.6061\nEpoch [25/50], Step [800/1265], Loss: 1.7349\nEpoch [25/50], Step [900/1265], Loss: 1.7787\nEpoch [25/50], Step [1000/1265], Loss: 1.6988\nEpoch [25/50], Step [1100/1265], Loss: 1.7691\nEpoch [25/50], Step [1200/1265], Loss: 1.7585\nEpoch [25/50] completed.\nEpoch [26/50], Step [100/1265], Loss: 1.6525\nEpoch [26/50], Step [200/1265], Loss: 1.5000\nEpoch [26/50], Step [300/1265], Loss: 1.7140\nEpoch [26/50], Step [400/1265], Loss: 1.6838\nEpoch [26/50], Step [500/1265], Loss: 1.6918\nEpoch [26/50], Step [600/1265], Loss: 1.4829\nEpoch [26/50], Step [700/1265], Loss: 1.5272\nEpoch [26/50], Step [800/1265], Loss: 1.6078\nEpoch [26/50], Step [900/1265], Loss: 1.7176\nEpoch [26/50], Step [1000/1265], Loss: 1.5896\nEpoch [26/50], Step [1100/1265], Loss: 1.6533\nEpoch [26/50], Step [1200/1265], Loss: 1.5736\nEpoch [26/50] completed.\nEpoch [27/50], Step [100/1265], Loss: 1.4705\nEpoch [27/50], Step [200/1265], Loss: 1.5378\nEpoch [27/50], Step [300/1265], Loss: 1.4828\nEpoch [27/50], Step [400/1265], Loss: 1.5291\nEpoch [27/50], Step [500/1265], Loss: 1.5220\nEpoch [27/50], Step [600/1265], Loss: 1.4559\nEpoch [27/50], Step [700/1265], Loss: 1.4863\nEpoch [27/50], Step [800/1265], Loss: 1.5174\nEpoch [27/50], Step [900/1265], Loss: 1.4589\nEpoch [27/50], Step [1000/1265], Loss: 1.6102\nEpoch [27/50], Step [1100/1265], Loss: 1.5804\nEpoch [27/50], Step [1200/1265], Loss: 1.5486\nEpoch [27/50] completed.\nEpoch [28/50], Step [100/1265], Loss: 1.5286\nEpoch [28/50], Step [200/1265], Loss: 1.4019\nEpoch [28/50], Step [300/1265], Loss: 1.5244\nEpoch [28/50], Step [400/1265], Loss: 1.5905\nEpoch [28/50], Step [500/1265], Loss: 1.5506\nEpoch [28/50], Step [600/1265], Loss: 1.5608\nEpoch [28/50], Step [700/1265], Loss: 1.4144\nEpoch [28/50], Step [800/1265], Loss: 1.5254\nEpoch [28/50], Step [900/1265], Loss: 1.5751\nEpoch [28/50], Step [1000/1265], Loss: 1.5461\nEpoch [28/50], Step [1100/1265], Loss: 1.6350\nEpoch [28/50], Step [1200/1265], Loss: 1.6070\nEpoch [28/50] completed.\nEpoch [29/50], Step [100/1265], Loss: 1.4370\nEpoch [29/50], Step [200/1265], Loss: 1.5415\nEpoch [29/50], Step [300/1265], Loss: 1.3661\nEpoch [29/50], Step [400/1265], Loss: 1.4824\nEpoch [29/50], Step [500/1265], Loss: 1.4621\nEpoch [29/50], Step [600/1265], Loss: 1.3933\nEpoch [29/50], Step [700/1265], Loss: 1.6109\nEpoch [29/50], Step [800/1265], Loss: 1.4531\nEpoch [29/50], Step [900/1265], Loss: 1.5443\nEpoch [29/50], Step [1000/1265], Loss: 1.4283\nEpoch [29/50], Step [1100/1265], Loss: 1.5024\nEpoch [29/50], Step [1200/1265], Loss: 1.5397\nEpoch [29/50] completed.\nEpoch [30/50], Step [100/1265], Loss: 1.2517\nEpoch [30/50], Step [200/1265], Loss: 1.3942\nEpoch [30/50], Step [300/1265], Loss: 1.4227\nEpoch [30/50], Step [400/1265], Loss: 1.5055\nEpoch [30/50], Step [500/1265], Loss: 1.5028\nEpoch [30/50], Step [600/1265], Loss: 1.4020\nEpoch [30/50], Step [700/1265], Loss: 1.2760\nEpoch [30/50], Step [800/1265], Loss: 1.6312\nEpoch [30/50], Step [900/1265], Loss: 1.4675\nEpoch [30/50], Step [1000/1265], Loss: 1.5818\nEpoch [30/50], Step [1100/1265], Loss: 1.4923\nEpoch [30/50], Step [1200/1265], Loss: 1.4946\nEpoch [30/50] completed.\nEpoch [31/50], Step [100/1265], Loss: 1.3699\nEpoch [31/50], Step [200/1265], Loss: 1.3865\nEpoch [31/50], Step [300/1265], Loss: 1.3408\nEpoch [31/50], Step [400/1265], Loss: 1.4602\nEpoch [31/50], Step [500/1265], Loss: 1.3844\nEpoch [31/50], Step [600/1265], Loss: 1.4554\nEpoch [31/50], Step [700/1265], Loss: 1.6022\nEpoch [31/50], Step [800/1265], Loss: 1.4759\nEpoch [31/50], Step [900/1265], Loss: 1.3206\nEpoch [31/50], Step [1000/1265], Loss: 1.5706\nEpoch [31/50], Step [1100/1265], Loss: 1.4369\nEpoch [31/50], Step [1200/1265], Loss: 1.3443\nEpoch [31/50] completed.\nEpoch [32/50], Step [100/1265], Loss: 1.3335\nEpoch [32/50], Step [200/1265], Loss: 1.3838\nEpoch [32/50], Step [300/1265], Loss: 1.2757\nEpoch [32/50], Step [400/1265], Loss: 1.3542\nEpoch [32/50], Step [500/1265], Loss: 1.3772\nEpoch [32/50], Step [600/1265], Loss: 1.3460\nEpoch [32/50], Step [700/1265], Loss: 1.3858\nEpoch [32/50], Step [800/1265], Loss: 1.4009\nEpoch [32/50], Step [900/1265], Loss: 1.4666\nEpoch [32/50], Step [1000/1265], Loss: 1.5806\nEpoch [32/50], Step [1100/1265], Loss: 1.3815\nEpoch [32/50], Step [1200/1265], Loss: 1.5927\nEpoch [32/50] completed.\nEpoch [33/50], Step [100/1265], Loss: 1.3220\nEpoch [33/50], Step [200/1265], Loss: 1.3674\nEpoch [33/50], Step [300/1265], Loss: 1.3419\nEpoch [33/50], Step [400/1265], Loss: 1.3632\nEpoch [33/50], Step [500/1265], Loss: 1.4166\nEpoch [33/50], Step [600/1265], Loss: 1.4566\nEpoch [33/50], Step [700/1265], Loss: 1.3870\nEpoch [33/50], Step [800/1265], Loss: 1.3342\nEpoch [33/50], Step [900/1265], Loss: 1.4456\nEpoch [33/50], Step [1000/1265], Loss: 1.3871\nEpoch [33/50], Step [1100/1265], Loss: 1.5059\nEpoch [33/50], Step [1200/1265], Loss: 1.4832\nEpoch [33/50] completed.\nEpoch [34/50], Step [100/1265], Loss: 1.1600\nEpoch [34/50], Step [200/1265], Loss: 1.2255\nEpoch [34/50], Step [300/1265], Loss: 1.1842\nEpoch [34/50], Step [400/1265], Loss: 1.4371\nEpoch [34/50], Step [500/1265], Loss: 1.3336\nEpoch [34/50], Step [600/1265], Loss: 1.4347\nEpoch [34/50], Step [700/1265], Loss: 1.4538\nEpoch [34/50], Step [800/1265], Loss: 1.2754\nEpoch [34/50], Step [900/1265], Loss: 1.3598\nEpoch [34/50], Step [1000/1265], Loss: 1.4555\nEpoch [34/50], Step [1100/1265], Loss: 1.5138\nEpoch [34/50], Step [1200/1265], Loss: 1.2846\nEpoch [34/50] completed.\nEpoch [35/50], Step [100/1265], Loss: 1.2360\nEpoch [35/50], Step [200/1265], Loss: 1.2591\nEpoch [35/50], Step [300/1265], Loss: 1.3007\nEpoch [35/50], Step [400/1265], Loss: 1.3529\nEpoch [35/50], Step [500/1265], Loss: 1.3980\nEpoch [35/50], Step [600/1265], Loss: 1.3695\nEpoch [35/50], Step [700/1265], Loss: 1.4488\nEpoch [35/50], Step [800/1265], Loss: 1.2987\nEpoch [35/50], Step [900/1265], Loss: 1.2596\nEpoch [35/50], Step [1000/1265], Loss: 1.3073\nEpoch [35/50], Step [1100/1265], Loss: 1.3124\nEpoch [35/50], Step [1200/1265], Loss: 1.4258\nEpoch [35/50] completed.\nEpoch [36/50], Step [100/1265], Loss: 1.2825\nEpoch [36/50], Step [200/1265], Loss: 1.2533\nEpoch [36/50], Step [300/1265], Loss: 1.3592\nEpoch [36/50], Step [400/1265], Loss: 1.2463\nEpoch [36/50], Step [500/1265], Loss: 1.3185\nEpoch [36/50], Step [600/1265], Loss: 1.3032\nEpoch [36/50], Step [700/1265], Loss: 1.2622\nEpoch [36/50], Step [800/1265], Loss: 1.3408\nEpoch [36/50], Step [900/1265], Loss: 1.2665\nEpoch [36/50], Step [1000/1265], Loss: 1.3189\nEpoch [36/50], Step [1100/1265], Loss: 1.1779\nEpoch [36/50], Step [1200/1265], Loss: 1.3506\nEpoch [36/50] completed.\nEpoch [37/50], Step [100/1265], Loss: 1.1784\nEpoch [37/50], Step [200/1265], Loss: 1.1145\nEpoch [37/50], Step [300/1265], Loss: 1.1981\nEpoch [37/50], Step [400/1265], Loss: 1.3530\nEpoch [37/50], Step [500/1265], Loss: 1.2108\nEpoch [37/50], Step [600/1265], Loss: 1.2495\nEpoch [37/50], Step [700/1265], Loss: 1.2837\nEpoch [37/50], Step [800/1265], Loss: 1.2680\nEpoch [37/50], Step [900/1265], Loss: 1.2603\nEpoch [37/50], Step [1000/1265], Loss: 1.2335\nEpoch [37/50], Step [1100/1265], Loss: 1.2359\nEpoch [37/50], Step [1200/1265], Loss: 1.1860\nEpoch [37/50] completed.\nEpoch [38/50], Step [100/1265], Loss: 1.1557\nEpoch [38/50], Step [200/1265], Loss: 1.2127\nEpoch [38/50], Step [300/1265], Loss: 1.1157\nEpoch [38/50], Step [400/1265], Loss: 1.2567\nEpoch [38/50], Step [500/1265], Loss: 1.2766\nEpoch [38/50], Step [600/1265], Loss: 1.3134\nEpoch [38/50], Step [700/1265], Loss: 1.2340\nEpoch [38/50], Step [800/1265], Loss: 1.1835\nEpoch [38/50], Step [900/1265], Loss: 1.0931\nEpoch [38/50], Step [1000/1265], Loss: 1.2022\nEpoch [38/50], Step [1100/1265], Loss: 1.2817\nEpoch [38/50], Step [1200/1265], Loss: 1.2757\nEpoch [38/50] completed.\nEpoch [39/50], Step [100/1265], Loss: 1.1650\nEpoch [39/50], Step [200/1265], Loss: 1.1230\nEpoch [39/50], Step [300/1265], Loss: 1.1652\nEpoch [39/50], Step [400/1265], Loss: 1.1349\nEpoch [39/50], Step [500/1265], Loss: 1.1766\nEpoch [39/50], Step [600/1265], Loss: 1.1143\nEpoch [39/50], Step [700/1265], Loss: 1.0344\nEpoch [39/50], Step [800/1265], Loss: 1.1718\nEpoch [39/50], Step [900/1265], Loss: 1.1777\nEpoch [39/50], Step [1000/1265], Loss: 1.2498\nEpoch [39/50], Step [1100/1265], Loss: 1.2119\nEpoch [39/50], Step [1200/1265], Loss: 1.4236\nEpoch [39/50] completed.\nEpoch [40/50], Step [100/1265], Loss: 1.1233\nEpoch [40/50], Step [200/1265], Loss: 1.0611\nEpoch [40/50], Step [300/1265], Loss: 1.1731\nEpoch [40/50], Step [400/1265], Loss: 1.2121\nEpoch [40/50], Step [500/1265], Loss: 1.2254\nEpoch [40/50], Step [600/1265], Loss: 1.2332\nEpoch [40/50], Step [700/1265], Loss: 1.0962\nEpoch [40/50], Step [800/1265], Loss: 1.2149\nEpoch [40/50], Step [900/1265], Loss: 1.2057\nEpoch [40/50], Step [1000/1265], Loss: 1.2143\nEpoch [40/50], Step [1100/1265], Loss: 1.2238\nEpoch [40/50], Step [1200/1265], Loss: 1.2191\nEpoch [40/50] completed.\nEpoch [41/50], Step [100/1265], Loss: 1.0862\nEpoch [41/50], Step [200/1265], Loss: 1.1292\nEpoch [41/50], Step [300/1265], Loss: 1.1037\nEpoch [41/50], Step [400/1265], Loss: 1.1532\nEpoch [41/50], Step [500/1265], Loss: 1.1528\nEpoch [41/50], Step [600/1265], Loss: 1.2456\nEpoch [41/50], Step [700/1265], Loss: 1.0264\nEpoch [41/50], Step [800/1265], Loss: 1.1361\nEpoch [41/50], Step [900/1265], Loss: 1.0168\nEpoch [41/50], Step [1000/1265], Loss: 1.2289\nEpoch [41/50], Step [1100/1265], Loss: 1.2226\nEpoch [41/50], Step [1200/1265], Loss: 1.1739\nEpoch [41/50] completed.\nEpoch [42/50], Step [100/1265], Loss: 1.1526\nEpoch [42/50], Step [200/1265], Loss: 1.1196\nEpoch [42/50], Step [300/1265], Loss: 0.9863\nEpoch [42/50], Step [400/1265], Loss: 1.1424\nEpoch [42/50], Step [500/1265], Loss: 1.0972\nEpoch [42/50], Step [600/1265], Loss: 1.2365\nEpoch [42/50], Step [700/1265], Loss: 1.1981\nEpoch [42/50], Step [800/1265], Loss: 1.1207\nEpoch [42/50], Step [900/1265], Loss: 1.1000\nEpoch [42/50], Step [1000/1265], Loss: 1.2213\nEpoch [42/50], Step [1100/1265], Loss: 1.0863\nEpoch [42/50], Step [1200/1265], Loss: 1.1898\nEpoch [42/50] completed.\nEpoch [43/50], Step [100/1265], Loss: 0.9960\nEpoch [43/50], Step [200/1265], Loss: 1.0899\nEpoch [43/50], Step [300/1265], Loss: 1.1296\nEpoch [43/50], Step [400/1265], Loss: 1.0834\nEpoch [43/50], Step [500/1265], Loss: 1.0838\nEpoch [43/50], Step [600/1265], Loss: 1.1342\nEpoch [43/50], Step [700/1265], Loss: 1.0085\nEpoch [43/50], Step [800/1265], Loss: 1.0861\nEpoch [43/50], Step [900/1265], Loss: 1.0236\nEpoch [43/50], Step [1000/1265], Loss: 1.1303\nEpoch [43/50], Step [1100/1265], Loss: 1.2025\nEpoch [43/50], Step [1200/1265], Loss: 1.1402\nEpoch [43/50] completed.\nEpoch [44/50], Step [100/1265], Loss: 1.0185\nEpoch [44/50], Step [200/1265], Loss: 1.0190\nEpoch [44/50], Step [300/1265], Loss: 1.0068\nEpoch [44/50], Step [400/1265], Loss: 0.9886\nEpoch [44/50], Step [500/1265], Loss: 1.0254\nEpoch [44/50], Step [600/1265], Loss: 1.1517\nEpoch [44/50], Step [700/1265], Loss: 1.1064\nEpoch [44/50], Step [800/1265], Loss: 1.1156\nEpoch [44/50], Step [900/1265], Loss: 1.1919\nEpoch [44/50], Step [1000/1265], Loss: 1.0697\nEpoch [44/50], Step [1100/1265], Loss: 1.1446\nEpoch [44/50], Step [1200/1265], Loss: 1.1863\nEpoch [44/50] completed.\nEpoch [45/50], Step [100/1265], Loss: 0.9699\nEpoch [45/50], Step [200/1265], Loss: 0.9191\nEpoch [45/50], Step [300/1265], Loss: 1.0191\nEpoch [45/50], Step [400/1265], Loss: 1.0032\nEpoch [45/50], Step [500/1265], Loss: 1.0587\nEpoch [45/50], Step [600/1265], Loss: 1.1022\nEpoch [45/50], Step [700/1265], Loss: 1.0888\nEpoch [45/50], Step [800/1265], Loss: 1.1186\nEpoch [45/50], Step [900/1265], Loss: 1.1229\nEpoch [45/50], Step [1000/1265], Loss: 1.1678\nEpoch [45/50], Step [1100/1265], Loss: 1.1443\nEpoch [45/50], Step [1200/1265], Loss: 1.1334\nEpoch [45/50] completed.\nEpoch [46/50], Step [100/1265], Loss: 1.0092\nEpoch [46/50], Step [200/1265], Loss: 1.0768\nEpoch [46/50], Step [300/1265], Loss: 0.9612\nEpoch [46/50], Step [400/1265], Loss: 1.0035\nEpoch [46/50], Step [500/1265], Loss: 0.9859\nEpoch [46/50], Step [600/1265], Loss: 1.0577\nEpoch [46/50], Step [700/1265], Loss: 1.0285\nEpoch [46/50], Step [800/1265], Loss: 1.0319\nEpoch [46/50], Step [900/1265], Loss: 1.0918\nEpoch [46/50], Step [1000/1265], Loss: 1.0906\nEpoch [46/50], Step [1100/1265], Loss: 1.0499\nEpoch [46/50], Step [1200/1265], Loss: 0.9738\nEpoch [46/50] completed.\nEpoch [47/50], Step [100/1265], Loss: 0.9248\nEpoch [47/50], Step [200/1265], Loss: 1.0645\nEpoch [47/50], Step [300/1265], Loss: 1.0444\nEpoch [47/50], Step [400/1265], Loss: 0.9336\nEpoch [47/50], Step [500/1265], Loss: 1.0530\nEpoch [47/50], Step [600/1265], Loss: 1.0306\nEpoch [47/50], Step [700/1265], Loss: 0.9617\nEpoch [47/50], Step [800/1265], Loss: 1.0586\nEpoch [47/50], Step [900/1265], Loss: 1.2466\nEpoch [47/50], Step [1000/1265], Loss: 1.1768\nEpoch [47/50], Step [1100/1265], Loss: 1.1230\nEpoch [47/50], Step [1200/1265], Loss: 0.9781\nEpoch [47/50] completed.\nEpoch [48/50], Step [100/1265], Loss: 0.9159\nEpoch [48/50], Step [200/1265], Loss: 0.8840\nEpoch [48/50], Step [300/1265], Loss: 0.9886\nEpoch [48/50], Step [400/1265], Loss: 0.9568\nEpoch [48/50], Step [500/1265], Loss: 1.0531\nEpoch [48/50], Step [600/1265], Loss: 1.0018\nEpoch [48/50], Step [700/1265], Loss: 0.9923\nEpoch [48/50], Step [800/1265], Loss: 1.0930\nEpoch [48/50], Step [900/1265], Loss: 1.0514\nEpoch [48/50], Step [1000/1265], Loss: 1.0272\nEpoch [48/50], Step [1100/1265], Loss: 1.1487\nEpoch [48/50], Step [1200/1265], Loss: 1.1101\nEpoch [48/50] completed.\nEpoch [49/50], Step [100/1265], Loss: 0.8296\nEpoch [49/50], Step [200/1265], Loss: 0.9122\nEpoch [49/50], Step [300/1265], Loss: 0.9513\nEpoch [49/50], Step [400/1265], Loss: 1.0223\nEpoch [49/50], Step [500/1265], Loss: 0.9968\nEpoch [49/50], Step [600/1265], Loss: 1.0971\nEpoch [49/50], Step [700/1265], Loss: 1.0678\nEpoch [49/50], Step [800/1265], Loss: 1.0268\nEpoch [49/50], Step [900/1265], Loss: 0.9733\nEpoch [49/50], Step [1000/1265], Loss: 0.9844\nEpoch [49/50], Step [1100/1265], Loss: 1.1965\nEpoch [49/50], Step [1200/1265], Loss: 0.9891\nEpoch [49/50] completed.\nEpoch [50/50], Step [100/1265], Loss: 0.9330\nEpoch [50/50], Step [200/1265], Loss: 0.9052\nEpoch [50/50], Step [300/1265], Loss: 0.8190\nEpoch [50/50], Step [400/1265], Loss: 1.0511\nEpoch [50/50], Step [500/1265], Loss: 0.9500\nEpoch [50/50], Step [600/1265], Loss: 0.9793\nEpoch [50/50], Step [700/1265], Loss: 0.9829\nEpoch [50/50], Step [800/1265], Loss: 0.8535\nEpoch [50/50], Step [900/1265], Loss: 0.9979\nEpoch [50/50], Step [1000/1265], Loss: 0.9561\nEpoch [50/50], Step [1100/1265], Loss: 0.8972\nEpoch [50/50], Step [1200/1265], Loss: 0.9829\nEpoch [50/50] completed.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save(encoder.state_dict(), 'encoder_final.pth')\ntorch.save(decoder.state_dict(), 'decoder_final.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:40:59.514096Z","iopub.execute_input":"2025-05-03T20:40:59.514361Z","iopub.status.idle":"2025-05-03T20:40:59.708241Z","shell.execute_reply.started":"2025-05-03T20:40:59.514345Z","shell.execute_reply":"2025-05-03T20:40:59.707520Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"import pickle\nwith open('vocabulary_final.pkl', 'wb') as f:\n    pickle.dump(vocab, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:40:59.709038Z","iopub.execute_input":"2025-05-03T20:40:59.709216Z","iopub.status.idle":"2025-05-03T20:40:59.715696Z","shell.execute_reply.started":"2025-05-03T20:40:59.709202Z","shell.execute_reply":"2025-05-03T20:40:59.715165Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"> Inference code\n```python\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderLSTM(embed_size, hidden_size, len(vocab)).to(device)\nencoder.load_state_dict(torch.load('encoder.pth'))\ndecoder.load_state_dict(torch.load('decoder.pth'))\nwith open('vocabulary.pkl', 'rb') as f:\n    vocab = pickle.load(f)\n```","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport pickle\n\n# Vocabulary and Model Definitions (as before)\nclass Vocabulary:\n    def __init__(self, freq_threshold=1):\n        self.freq_threshold = freq_threshold\n        self.word2idx = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.idx2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.word_freq = {}\n        self.idx = 4\n\n    def build_vocabulary(self, sentence_list):\n        for sentence in sentence_list:\n            for word in word_tokenize(sentence.lower()):\n                self.word_freq[word] = self.word_freq.get(word, 0) + 1\n                if self.word_freq[word] == self.freq_threshold:\n                    self.word2idx[word] = self.idx\n                    self.idx2word[self.idx] = word\n                    self.idx += 1\n\n    def numericalize(self, text):\n        tokenized = word_tokenize(text.lower())\n        return [self.word2idx.get(word, self.word2idx[\"<UNK>\"]) for word in tokenized]\n\n    def __call__(self, word):\n        return self.word2idx.get(word, self.word2idx[\"<UNK>\"])\n\n    def __len__(self):\n        return len(self.word2idx)\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n\n    def forward(self, images):\n        with torch.no_grad():\n            features = self.resnet(images) # Keep the 4D tensor\n            features = features.view(features.size(0), -1) # Flatten the spatial dimensions while keeping the batch\n        features = self.linear(features)\n        features = self.bn(features)\n        return features\n\nclass DecoderLSTM(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions):\n        embeddings = self.embed(captions[:, :-1])\n        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n        hiddens, _ = self.lstm(inputs)\n        outputs = self.linear(hiddens)\n        return outputs\n\ndef generate_caption(encoder, decoder, image, vocab, max_len=20):\n    encoder.eval()\n    decoder.eval()\n    with torch.no_grad():\n        feature = encoder(image)\n        inputs = feature\n        caption = []\n        states = None\n        input_word = torch.tensor([[vocab('<SOS>')]]).to(device)\n        for _ in range(max_len):\n            embedded = decoder.embed(input_word).squeeze(1).unsqueeze(0)\n            inputs, states = decoder.lstm(embedded, states)\n            outputs = decoder.linear(inputs.squeeze(1))\n            predicted = outputs.argmax(1)\n            word = vocab.idx2word[predicted.item()]\n            if word == '<EOS>':\n                break\n            caption.append(word)\n            input_word = predicted.unsqueeze(0)\n    return ' '.join(caption)\n\n# Load Vocabulary\nwith open('/kaggle/working/vocabulary_final.pkl', 'rb') as f:\n    vocab = pickle.load(f)\n\n# Set Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize and Load Models\nembed_size = 256\nhidden_size = 512\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderLSTM(embed_size, hidden_size, len(vocab)).to(device)\nencoder.load_state_dict(torch.load('/kaggle/working/encoder_final.pth'))\ndecoder.load_state_dict(torch.load('/kaggle/working/decoder_final.pth'))\nencoder.eval()\ndecoder.eval()\n\n# Image Transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\ndef load_and_transform_image(image_path, transform=None):\n    image = Image.open(image_path).convert(\"RGB\")\n    if transform is not None:\n        image = transform(image).unsqueeze(0).to(device)\n    return image\n\n# Path to your new image\nimage_path = '/kaggle/input/image3/image.png'\nimage = load_and_transform_image(image_path, transform)\n\n# Generate Caption\ngenerated_caption = generate_caption(encoder, decoder, image, vocab)\nprint(f\"Generated Caption: {generated_caption}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T20:57:26.879811Z","iopub.execute_input":"2025-05-03T20:57:26.880357Z","iopub.status.idle":"2025-05-03T20:57:27.662845Z","shell.execute_reply.started":"2025-05-03T20:57:26.880325Z","shell.execute_reply":"2025-05-03T20:57:27.662048Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Generated Caption: two are down , with children down their and upside .\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1175783703.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  encoder.load_state_dict(torch.load('/kaggle/working/encoder_final.pth'))\n/tmp/ipykernel_31/1175783703.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  decoder.load_state_dict(torch.load('/kaggle/working/decoder_final.pth'))\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}