{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzroClZSpwVxjPHDbthmnj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kush-Singh-26/NLP/blob/main/ChargenerateRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VJqrQBoRqqR8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpNsWlwNqvJK",
        "outputId": "5c6027e1-090a-4c69-c44c-7c2ec2dadfea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-03 19:41:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-05-03 19:41:34 (24.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
        "encoded_text = torch.tensor([char2idx[c] for c in text], dtype=torch.long)"
      ],
      "metadata": {
        "id": "3hHWoNisqw97"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 100\n",
        "batch_size = 64\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.002\n",
        "num_epochs = 500"
      ],
      "metadata": {
        "id": "ahIqoqg_q0Mx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(encoded_text, block_size, batch_size):\n",
        "    ix = torch.randint(0, len(encoded_text) - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([encoded_text[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([encoded_text[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "bKzcgrkYq2F3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embed(x)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n"
      ],
      "metadata": {
        "id": "QDW23LCmq3XU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(vocab_size, hidden_size, num_layers)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"Training started...\")\n",
        "for epoch in range(num_epochs):\n",
        "    x_batch, y_batch = get_batch(encoded_text, block_size, batch_size)\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    logits, _ = model(x_batch)\n",
        "    loss = loss_fn(logits.view(-1, vocab_size), y_batch.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4HE6zVNq5K5",
        "outputId": "b0cf9b0f-ecdb-493c-cb09-70aadb1b404a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "Epoch 2/500, Loss: 3.7997\n",
            "Epoch 4/500, Loss: 3.1649\n",
            "Epoch 6/500, Loss: 2.9249\n",
            "Epoch 8/500, Loss: 2.7971\n",
            "Epoch 10/500, Loss: 2.6515\n",
            "Epoch 12/500, Loss: 2.5842\n",
            "Epoch 14/500, Loss: 2.5096\n",
            "Epoch 16/500, Loss: 2.4375\n",
            "Epoch 18/500, Loss: 2.3958\n",
            "Epoch 20/500, Loss: 2.3650\n",
            "Epoch 22/500, Loss: 2.3448\n",
            "Epoch 24/500, Loss: 2.3034\n",
            "Epoch 26/500, Loss: 2.2964\n",
            "Epoch 28/500, Loss: 2.2738\n",
            "Epoch 30/500, Loss: 2.2388\n",
            "Epoch 32/500, Loss: 2.2241\n",
            "Epoch 34/500, Loss: 2.1808\n",
            "Epoch 36/500, Loss: 2.1766\n",
            "Epoch 38/500, Loss: 2.1217\n",
            "Epoch 40/500, Loss: 2.1111\n",
            "Epoch 42/500, Loss: 2.0957\n",
            "Epoch 44/500, Loss: 2.1444\n",
            "Epoch 46/500, Loss: 2.1087\n",
            "Epoch 48/500, Loss: 2.0835\n",
            "Epoch 50/500, Loss: 2.0802\n",
            "Epoch 52/500, Loss: 2.0588\n",
            "Epoch 54/500, Loss: 2.0814\n",
            "Epoch 56/500, Loss: 2.0406\n",
            "Epoch 58/500, Loss: 1.9893\n",
            "Epoch 60/500, Loss: 2.0542\n",
            "Epoch 62/500, Loss: 2.0094\n",
            "Epoch 64/500, Loss: 2.0056\n",
            "Epoch 66/500, Loss: 1.9934\n",
            "Epoch 68/500, Loss: 1.9296\n",
            "Epoch 70/500, Loss: 1.9320\n",
            "Epoch 72/500, Loss: 1.9687\n",
            "Epoch 74/500, Loss: 1.9870\n",
            "Epoch 76/500, Loss: 1.9400\n",
            "Epoch 78/500, Loss: 1.9389\n",
            "Epoch 80/500, Loss: 1.9546\n",
            "Epoch 82/500, Loss: 1.9650\n",
            "Epoch 84/500, Loss: 1.9431\n",
            "Epoch 86/500, Loss: 1.9290\n",
            "Epoch 88/500, Loss: 1.8495\n",
            "Epoch 90/500, Loss: 1.8513\n",
            "Epoch 92/500, Loss: 1.8767\n",
            "Epoch 94/500, Loss: 1.9125\n",
            "Epoch 96/500, Loss: 1.8506\n",
            "Epoch 98/500, Loss: 1.8654\n",
            "Epoch 100/500, Loss: 1.8400\n",
            "Epoch 102/500, Loss: 1.8315\n",
            "Epoch 104/500, Loss: 1.8850\n",
            "Epoch 106/500, Loss: 1.8574\n",
            "Epoch 108/500, Loss: 1.8414\n",
            "Epoch 110/500, Loss: 1.8240\n",
            "Epoch 112/500, Loss: 1.7968\n",
            "Epoch 114/500, Loss: 1.8226\n",
            "Epoch 116/500, Loss: 1.7906\n",
            "Epoch 118/500, Loss: 1.7982\n",
            "Epoch 120/500, Loss: 1.7758\n",
            "Epoch 122/500, Loss: 1.8453\n",
            "Epoch 124/500, Loss: 1.8235\n",
            "Epoch 126/500, Loss: 1.7507\n",
            "Epoch 128/500, Loss: 1.7711\n",
            "Epoch 130/500, Loss: 1.7881\n",
            "Epoch 132/500, Loss: 1.7409\n",
            "Epoch 134/500, Loss: 1.7667\n",
            "Epoch 136/500, Loss: 1.7566\n",
            "Epoch 138/500, Loss: 1.7875\n",
            "Epoch 140/500, Loss: 1.7878\n",
            "Epoch 142/500, Loss: 1.7671\n",
            "Epoch 144/500, Loss: 1.7878\n",
            "Epoch 146/500, Loss: 1.7401\n",
            "Epoch 148/500, Loss: 1.7284\n",
            "Epoch 150/500, Loss: 1.7113\n",
            "Epoch 152/500, Loss: 1.7231\n",
            "Epoch 154/500, Loss: 1.7276\n",
            "Epoch 156/500, Loss: 1.7755\n",
            "Epoch 158/500, Loss: 1.7463\n",
            "Epoch 160/500, Loss: 1.7193\n",
            "Epoch 162/500, Loss: 1.7252\n",
            "Epoch 164/500, Loss: 1.7405\n",
            "Epoch 166/500, Loss: 1.7017\n",
            "Epoch 168/500, Loss: 1.7126\n",
            "Epoch 170/500, Loss: 1.7632\n",
            "Epoch 172/500, Loss: 1.7063\n",
            "Epoch 174/500, Loss: 1.7231\n",
            "Epoch 176/500, Loss: 1.6638\n",
            "Epoch 178/500, Loss: 1.7045\n",
            "Epoch 180/500, Loss: 1.6952\n",
            "Epoch 182/500, Loss: 1.7051\n",
            "Epoch 184/500, Loss: 1.6964\n",
            "Epoch 186/500, Loss: 1.6568\n",
            "Epoch 188/500, Loss: 1.7001\n",
            "Epoch 190/500, Loss: 1.6985\n",
            "Epoch 192/500, Loss: 1.6492\n",
            "Epoch 194/500, Loss: 1.6648\n",
            "Epoch 196/500, Loss: 1.7004\n",
            "Epoch 198/500, Loss: 1.6529\n",
            "Epoch 200/500, Loss: 1.6638\n",
            "Epoch 202/500, Loss: 1.7131\n",
            "Epoch 204/500, Loss: 1.6563\n",
            "Epoch 206/500, Loss: 1.6688\n",
            "Epoch 208/500, Loss: 1.6737\n",
            "Epoch 210/500, Loss: 1.6335\n",
            "Epoch 212/500, Loss: 1.7162\n",
            "Epoch 214/500, Loss: 1.6186\n",
            "Epoch 216/500, Loss: 1.6557\n",
            "Epoch 218/500, Loss: 1.6778\n",
            "Epoch 220/500, Loss: 1.6637\n",
            "Epoch 222/500, Loss: 1.6476\n",
            "Epoch 224/500, Loss: 1.6432\n",
            "Epoch 226/500, Loss: 1.6466\n",
            "Epoch 228/500, Loss: 1.6666\n",
            "Epoch 230/500, Loss: 1.6452\n",
            "Epoch 232/500, Loss: 1.6575\n",
            "Epoch 234/500, Loss: 1.6611\n",
            "Epoch 236/500, Loss: 1.6504\n",
            "Epoch 238/500, Loss: 1.6162\n",
            "Epoch 240/500, Loss: 1.6522\n",
            "Epoch 242/500, Loss: 1.6071\n",
            "Epoch 244/500, Loss: 1.6239\n",
            "Epoch 246/500, Loss: 1.5941\n",
            "Epoch 248/500, Loss: 1.6515\n",
            "Epoch 250/500, Loss: 1.6226\n",
            "Epoch 252/500, Loss: 1.6303\n",
            "Epoch 254/500, Loss: 1.6232\n",
            "Epoch 256/500, Loss: 1.6401\n",
            "Epoch 258/500, Loss: 1.6097\n",
            "Epoch 260/500, Loss: 1.6251\n",
            "Epoch 262/500, Loss: 1.6447\n",
            "Epoch 264/500, Loss: 1.6187\n",
            "Epoch 266/500, Loss: 1.6091\n",
            "Epoch 268/500, Loss: 1.6529\n",
            "Epoch 270/500, Loss: 1.6358\n",
            "Epoch 272/500, Loss: 1.5710\n",
            "Epoch 274/500, Loss: 1.6007\n",
            "Epoch 276/500, Loss: 1.6195\n",
            "Epoch 278/500, Loss: 1.5830\n",
            "Epoch 280/500, Loss: 1.6268\n",
            "Epoch 282/500, Loss: 1.5980\n",
            "Epoch 284/500, Loss: 1.6215\n",
            "Epoch 286/500, Loss: 1.5918\n",
            "Epoch 288/500, Loss: 1.6028\n",
            "Epoch 290/500, Loss: 1.6337\n",
            "Epoch 292/500, Loss: 1.6303\n",
            "Epoch 294/500, Loss: 1.6005\n",
            "Epoch 296/500, Loss: 1.6098\n",
            "Epoch 298/500, Loss: 1.5525\n",
            "Epoch 300/500, Loss: 1.6245\n",
            "Epoch 302/500, Loss: 1.6016\n",
            "Epoch 304/500, Loss: 1.5511\n",
            "Epoch 306/500, Loss: 1.5703\n",
            "Epoch 308/500, Loss: 1.5885\n",
            "Epoch 310/500, Loss: 1.5777\n",
            "Epoch 312/500, Loss: 1.5479\n",
            "Epoch 314/500, Loss: 1.5614\n",
            "Epoch 316/500, Loss: 1.5764\n",
            "Epoch 318/500, Loss: 1.6103\n",
            "Epoch 320/500, Loss: 1.5564\n",
            "Epoch 322/500, Loss: 1.5693\n",
            "Epoch 324/500, Loss: 1.6136\n",
            "Epoch 326/500, Loss: 1.5744\n",
            "Epoch 328/500, Loss: 1.6070\n",
            "Epoch 330/500, Loss: 1.5485\n",
            "Epoch 332/500, Loss: 1.5460\n",
            "Epoch 334/500, Loss: 1.5966\n",
            "Epoch 336/500, Loss: 1.5490\n",
            "Epoch 338/500, Loss: 1.5508\n",
            "Epoch 340/500, Loss: 1.5539\n",
            "Epoch 342/500, Loss: 1.5705\n",
            "Epoch 344/500, Loss: 1.5259\n",
            "Epoch 346/500, Loss: 1.5460\n",
            "Epoch 348/500, Loss: 1.5640\n",
            "Epoch 350/500, Loss: 1.5986\n",
            "Epoch 352/500, Loss: 1.5704\n",
            "Epoch 354/500, Loss: 1.6014\n",
            "Epoch 356/500, Loss: 1.5218\n",
            "Epoch 358/500, Loss: 1.5572\n",
            "Epoch 360/500, Loss: 1.4915\n",
            "Epoch 362/500, Loss: 1.5372\n",
            "Epoch 364/500, Loss: 1.5536\n",
            "Epoch 366/500, Loss: 1.5832\n",
            "Epoch 368/500, Loss: 1.4728\n",
            "Epoch 370/500, Loss: 1.5530\n",
            "Epoch 372/500, Loss: 1.5456\n",
            "Epoch 374/500, Loss: 1.5970\n",
            "Epoch 376/500, Loss: 1.5426\n",
            "Epoch 378/500, Loss: 1.5374\n",
            "Epoch 380/500, Loss: 1.5650\n",
            "Epoch 382/500, Loss: 1.5549\n",
            "Epoch 384/500, Loss: 1.5793\n",
            "Epoch 386/500, Loss: 1.5305\n",
            "Epoch 388/500, Loss: 1.5013\n",
            "Epoch 390/500, Loss: 1.5059\n",
            "Epoch 392/500, Loss: 1.5833\n",
            "Epoch 394/500, Loss: 1.5285\n",
            "Epoch 396/500, Loss: 1.5262\n",
            "Epoch 398/500, Loss: 1.5341\n",
            "Epoch 400/500, Loss: 1.5435\n",
            "Epoch 402/500, Loss: 1.5514\n",
            "Epoch 404/500, Loss: 1.4972\n",
            "Epoch 406/500, Loss: 1.5034\n",
            "Epoch 408/500, Loss: 1.5074\n",
            "Epoch 410/500, Loss: 1.4739\n",
            "Epoch 412/500, Loss: 1.5166\n",
            "Epoch 414/500, Loss: 1.5205\n",
            "Epoch 416/500, Loss: 1.5639\n",
            "Epoch 418/500, Loss: 1.5133\n",
            "Epoch 420/500, Loss: 1.5434\n",
            "Epoch 422/500, Loss: 1.5361\n",
            "Epoch 424/500, Loss: 1.5199\n",
            "Epoch 426/500, Loss: 1.5070\n",
            "Epoch 428/500, Loss: 1.4994\n",
            "Epoch 430/500, Loss: 1.4842\n",
            "Epoch 432/500, Loss: 1.5343\n",
            "Epoch 434/500, Loss: 1.5157\n",
            "Epoch 436/500, Loss: 1.5808\n",
            "Epoch 438/500, Loss: 1.5065\n",
            "Epoch 440/500, Loss: 1.4938\n",
            "Epoch 442/500, Loss: 1.4793\n",
            "Epoch 444/500, Loss: 1.5006\n",
            "Epoch 446/500, Loss: 1.4811\n",
            "Epoch 448/500, Loss: 1.4927\n",
            "Epoch 450/500, Loss: 1.5176\n",
            "Epoch 452/500, Loss: 1.5195\n",
            "Epoch 454/500, Loss: 1.5118\n",
            "Epoch 456/500, Loss: 1.4989\n",
            "Epoch 458/500, Loss: 1.4741\n",
            "Epoch 460/500, Loss: 1.5162\n",
            "Epoch 462/500, Loss: 1.5219\n",
            "Epoch 464/500, Loss: 1.5301\n",
            "Epoch 466/500, Loss: 1.5336\n",
            "Epoch 468/500, Loss: 1.4722\n",
            "Epoch 470/500, Loss: 1.5313\n",
            "Epoch 472/500, Loss: 1.5290\n",
            "Epoch 474/500, Loss: 1.5153\n",
            "Epoch 476/500, Loss: 1.4810\n",
            "Epoch 478/500, Loss: 1.5340\n",
            "Epoch 480/500, Loss: 1.5317\n",
            "Epoch 482/500, Loss: 1.5027\n",
            "Epoch 484/500, Loss: 1.5549\n",
            "Epoch 486/500, Loss: 1.5424\n",
            "Epoch 488/500, Loss: 1.5068\n",
            "Epoch 490/500, Loss: 1.5127\n",
            "Epoch 492/500, Loss: 1.4695\n",
            "Epoch 494/500, Loss: 1.5354\n",
            "Epoch 496/500, Loss: 1.4994\n",
            "Epoch 498/500, Loss: 1.4729\n",
            "Epoch 500/500, Loss: 1.4900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_text='Once upon a ', length=300):\n",
        "    model.eval()\n",
        "    chars = list(start_text)\n",
        "    input_seq = torch.tensor([char2idx[c] for c in chars], dtype=torch.long).unsqueeze(0)\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(length):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        last_logits = output[0, -1, :]\n",
        "        probs = torch.softmax(last_logits, dim=0)\n",
        "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "        chars.append(idx2char[next_idx])\n",
        "        input_seq = torch.tensor([[next_idx]], dtype=torch.long)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "bls9ZLP2q7bS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nGenerated text:\")\n",
        "print(generate(model, start_text=\"Once upon a time,\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74U5b1BZq98o",
        "outputId": "e049c601-a895-453d-c678-eac38c5f2b99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "Once upon a time,\n",
            "So sleall but my Larcius,\n",
            "Seen for heaven?\n",
            "O ginders\n",
            "With rupe of Richurching of Warwipps and all this dauping I revight.\n",
            "\n",
            "Born, farther:\n",
            "Upon the wordst me, thensel my come hundrognautions mays and the unforce.\n",
            "Seelf anwill'tes, but prokes, sweet sir; here presence\n",
            "With my adveritions, my bots. I \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'char2idx': char2idx,\n",
        "    'idx2char': idx2char,\n",
        "    'vocab_size': vocab_size,\n",
        "    'hidden_size': hidden_size,\n",
        "    'num_layers': num_layers\n",
        "}, 'char_rnn_full.pt')\n"
      ],
      "metadata": {
        "id": "LUBaKtiYrGDZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "checkpoint = torch.load('char_rnn_full.pt')\n",
        "\n",
        "# Recreate model using saved hyperparameters\n",
        "model = CharRNN(\n",
        "    vocab_size=checkpoint['vocab_size'],\n",
        "    hidden_size=checkpoint['hidden_size'],\n",
        "    num_layers=checkpoint['num_layers']\n",
        ")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "char2idx = checkpoint['char2idx']\n",
        "idx2char = checkpoint['idx2char']\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "4rSvEyk0r_Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gpelMcjRr6pP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}